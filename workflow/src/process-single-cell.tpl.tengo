ll := import("@platforma-sdk/workflow-tengo:ll")
self := import("@platforma-sdk/workflow-tengo:tpl.light")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
assets := import("@platforma-sdk/workflow-tengo:assets")
exec := import("@platforma-sdk/workflow-tengo:exec")
maps := import("@platforma-sdk/workflow-tengo:maps")

json := import("json")

self.defineOutputs("abundanceTsv", "clonotypeTsv", "propertiesAPrimaryTsv", "propertiesASecondaryTsv", "propertiesBPrimaryTsv", "propertiesBSecondaryTsv")

scPreprocessingSw := assets.importSoftware("@platforma-open/milaboratories.mixcr-clonotyping-2.single-cell-scripts:preprocessing")
scGroupBuilderSw := assets.importSoftware("@platforma-open/milaboratories.mixcr-clonotyping-2.single-cell-scripts:sc-group-builder")
scOutputProcessingSw := assets.importSoftware("@platforma-open/milaboratories.mixcr-clonotyping-2.single-cell-scripts:output-processing")

ptablerSw := assets.importSoftware("@platforma-open/software-ptabler:main")

self.body(func(inputs) {
	byCellTagA := inputs[pConstants.VALUE_FIELD_NAME]
	inputDataMeta := byCellTagA.getDataAsJson()
	ll.assert(inputDataMeta.keyLength == 1, "unexpected number of aggregation axes")

	byCellTagB := inputs.byCellTagB
	propertiesA := inputs.propertiesA
	propertiesB := inputs.propertiesB

	mainAbundanceColumn := inputs.params.mainAbundanceColumn
	mainIsProductiveColumn := inputs.params.mainIsProductiveColumn

	/**
	 * Universal preprocessing step for A and B chain files
	 * @param byCellTag: Map<string:[sampleId], tsv_by_cell_tag>
	 * @return the output file
	 */
	preprocessByCell := func(byCellTag) {
		internalOutputFileName := "ptabler_output.tsv"
		ptablerSteps := []
		inputTableNames := []

		ptablerCmdBuilder := exec.builder().
			printErrStreamToStdout().
			software(ptablerSw)

		inputMap := byCellTag.inputs()
		maps.forEach(inputMap, func(sKey, inputFile) {
			key := json.decode(sKey)
			ll.assert(len(key) == 1, "preprocessByCell: byCellTag key should have one element, got %v", key)
			sampleId := key[0]

			fileNameForExecAndPtablerRead := "by_cell_" + sampleId + ".tsv"
			tableNameInPtabler := "by_cell_" + sampleId

			ptablerCmdBuilder.addFile(fileNameForExecAndPtablerRead, inputFile)

			ptablerSteps = append(ptablerSteps, {
				type: "read_csv",
				file: fileNameForExecAndPtablerRead,
				name: tableNameInPtabler,
				delimiter: "\t"
			})

			ptablerSteps = append(ptablerSteps, {
				type: "add_columns",
				table: tableNameInPtabler,
				columns: [
					{ name: "sampleId", expression: { type: "const", value: sampleId } }
				]
			})
			inputTableNames = append(inputTableNames, tableNameInPtabler)
		})

		ll.assert(len(inputTableNames) > 0, "No input files to process in preprocessByCell")

		columnsForInitialConcatenation := ["cellKey", "clonotypeKey", mainAbundanceColumn, "sampleId", mainIsProductiveColumn]

		ptablerSteps = append(ptablerSteps, {
			type: "concatenate",
			inputTables: inputTableNames,
			outputTable: "concatenated_data",
			columns: columnsForInitialConcatenation
		})

		ptablerSteps = append(ptablerSteps, {
			type: "add_columns",
			table: "concatenated_data",
			columns: [
				{
					name: "rawChainRank",
					expression: {
						type: "rank",
						partitionBy: [{ type: "col", name: "sampleId" }, { type: "col", name: "cellTag" }],
						orderBy: [{ type: "col", name: mainAbundanceColumn }],
						descending: true // Highest abundance gets rank 1
					}
				}
			]
		})

		ptablerSteps = append(ptablerSteps, {
			type: "filter",
			inputTable: "concatenated_data",
			outputTable: "concatenated_data_filtered_by_raw_rank",
			condition: {
				type: "le",
				lhs: { type: "col", name: "rawChainRank" },
				rhs: { type: "const", value: 2 }
			}
		})

		// Add chainRank: rank by isProductive (True first) then by abundance (desc) within sampleId+cellTag
		// For orderBy:
		// 1. isProductiveNumeric: "True" -> 0, "False" -> 1 (for ascending sort)
		// 2. negativeAbundance: mainAbundanceColumn * -1 (for ascending sort by this, meaning descending by original abundance)
		isProductiveNumericExpr := {
			type: "when_then_otherwise",
			conditions: [{
				when: { type: "eq", lhs: { type: "col", name: mainIsProductiveColumn }, rhs: { type: "const", value: "True" } },
				then: { type: "const", value: 0 }
			}],
			otherwise: { type: "const", value: 1 }
		}
		negativeAbundanceExpr := {
			type: "multiply",
			lhs: { type: "col", name: mainAbundanceColumn },
			rhs: { type: "const", value: -1 }
		}

		ptablerSteps = append(ptablerSteps, {
			type: "add_columns",
			table: "concatenated_data_filtered_by_raw_rank",
			columns: [
				{
					name: "chainRank",
					expression: {
						type: "rank",
						partitionBy: [{ type: "col", name: "sampleId" }, { type: "col", name: "cellTag" }],
						orderBy: [isProductiveNumericExpr, negativeAbundanceExpr],
						descending: false // Standard rank 1, 2, 3...
					}
				}
			]
		})

		ptablerSteps = append(ptablerSteps, {
			type: "write_csv",
			table: "concatenated_data_filtered_by_raw_rank",
			file: internalOutputFileName,
			delimiter: "\t"
		})

		ptablerWorkflow := {
			workflow: ptablerSteps
		}

		ptablerCmdBuilder.
			writeFile("workflow.json", json.encode(ptablerWorkflow)).
			arg("workflow.json").
			saveFile(internalOutputFileName)

		runResult := ptablerCmdBuilder.run()
		return runResult.getFile(internalOutputFileName)
	}

	chainAoutput := preprocessByCell(byCellTagA)
	chainBoutput := preprocessByCell(byCellTagB)

	scClonotypeBuilderCmd := exec.builder().
		printErrStreamToStdout().
		software(scGroupBuilderSw).
		addFile("chain_a_output.tsv", chainAoutput).
		addFile("chain_b_output.tsv", chainBoutput).
		arg("--only_full_clonotypes").
		arg("--chainAqwe").arg("chain_a_output.tsv").
		arg("--chainB").arg("chain_b_output.tsv").
		arg("--output_clonotype").arg("clonotype.tsv").
		arg("--output_cell").arg("abundance.tsv").
		saveFile("clonotype.tsv").
		saveFile("abundance.tsv")
	scClonotypeCmd := scClonotypeBuilderCmd.run()

	clonotypeTsv := scClonotypeCmd.getFile("clonotype.tsv")
	abundanceTsv := scClonotypeCmd.getFile("abundance.tsv")

	propertiesAFile := propertiesA.inputs()["[]"]
	propertiesBFile := propertiesB.inputs()["[]"]

	// Propagate scClonotypeKey to properties tables
	scOutputProcessingBuilderCmd := exec.builder().
		printErrStreamToStdout().
		software(scOutputProcessingSw).
		addFile("clonotype.tsv", clonotypeTsv).
		arg("--main_table").arg("clonotype.tsv").
		addFile("properties_a.tsv", propertiesAFile).
		addFile("properties_b.tsv", propertiesBFile).
		arg("--properties_a").arg("properties_a.tsv").
		arg("--properties_b").arg("properties_b.tsv").
		arg("--output_A1").arg("properties_a_primary.tsv").
		arg("--output_A2").arg("properties_a_secondary.tsv").
		arg("--output_B1").arg("properties_b_primary.tsv").
		arg("--output_B2").arg("properties_b_secondary.tsv").
		saveFile("properties_a_primary.tsv").
		saveFile("properties_a_secondary.tsv").
		saveFile("properties_b_primary.tsv").
		saveFile("properties_b_secondary.tsv")
	scOutputCmd := scOutputProcessingBuilderCmd.run()

	return {
		// must have sampleId and scClonotypeKey columns
		abundanceTsv: abundanceTsv,
		// used for aggregates (i.e. sampleCount)
		clonotypeTsv: clonotypeTsv,

		// must have scClonotypeKey columns
		propertiesAPrimaryTsv: scOutputCmd.getFile("properties_a_primary.tsv"),
		propertiesASecondaryTsv: scOutputCmd.getFile("properties_a_secondary.tsv"),
		propertiesBPrimaryTsv: scOutputCmd.getFile("properties_b_primary.tsv"),
		propertiesBSecondaryTsv: scOutputCmd.getFile("properties_b_secondary.tsv")
	}
})
