//tengo:hash_override AB9AC8BA-5FFC-40C5-B2F7-AC42181524F5

ll := import("@platforma-sdk/workflow-tengo:ll")
self := import("@platforma-sdk/workflow-tengo:tpl.light")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
assets := import("@platforma-sdk/workflow-tengo:assets")
exec := import("@platforma-sdk/workflow-tengo:exec")
maps := import("@platforma-sdk/workflow-tengo:maps")
slices := import("@platforma-sdk/workflow-tengo:slices")
units := import("@platforma-sdk/workflow-tengo:units")
pt := import("@platforma-sdk/workflow-tengo:pt")

clonotypeLabel := import(":clonotype-label")

json := import("json")
math := import("math")

self.defineOutputs("abundanceTsv", "clonotypeTsv",
	"propertiesAPrimaryTsv", "propertiesASecondaryTsv", "propertiesBPrimaryTsv", "propertiesBSecondaryTsv",
	"cellsTsv", "linkerTsv")

ptablerSw := assets.importSoftware("@platforma-open/milaboratories.software-ptabler:main")

self.body(func(inputs) {
	// ll.print("__THE_LOG__ PROCESS SINGLE CELL")

	byCellTagA := inputs[pConstants.VALUE_FIELD_NAME]
	inputDataMeta := byCellTagA.getDataAsJson()
	ll.assert(inputDataMeta.keyLength == 1, "unexpected number of aggregation axes")

	byCellTagB := inputs.byCellTagB
	propertiesA := inputs.propertiesA
	propertiesB := inputs.propertiesB

	mainAbundanceColumn := inputs.params.mainAbundanceColumn
	mainIsProductiveColumn := inputs.params.mainIsProductiveColumn

	schemaPerClonotypeNoAggregates := inputs.params.schemaPerClonotypeNoAggregates

	//
	// Preprocessing
	//

	/**
	 * Universal preprocessing step for A and B chain files
	 * @param byCellTag: Map<string:[sampleId], tsv_by_cell_tag>
	 * @return the output file
	 */
	preprocessByCell := func(byCellTag) {
		inputMap := byCellTag.inputs()
		numberOfSamples := len(inputMap)

		wf := pt.workflow().
			inMediumQueue().
			mem(int(math.max(numberOfSamples, 32)) * units.GiB).
			cpu(int(math.max(numberOfSamples, 16)))

		sampleDataFrames := []

		maps.forEach(inputMap, func(sKey, inputFile) {
			key := json.decode(sKey)
			ll.assert(len(key) == 1, "preprocessByCell: byCellTag key should have one element, got %v", key)
			sampleId := key[0]

			// Create a DataFrame for the current sample's file
			sampleDf := wf.frame(inputFile, {
				xsvType: "tsv",
				schema: [
					{ column: "cellKey", type: "String" },
					{ column: "clonotypeKey", type: "String" },
					{ column: mainAbundanceColumn, type: "Long" }, // Ensure mainAbundanceColumn is treated as Long
					{ column: mainIsProductiveColumn, type: "String" } // Keep as String for direct comparison
				],
				inferSchema: false // Disable further inference so columns not defined in schema are interpreted as strings
			}).withColumns(
				pt.col("cellKey"), pt.col("clonotypeKey"), pt.col(mainAbundanceColumn), pt.col(mainIsProductiveColumn)
			)

			// Add sampleId column
			sampleDfWithId := sampleDf.addColumns(
				pt.lit(sampleId).alias("sampleId")
			)
			sampleDataFrames = append(sampleDataFrames, sampleDfWithId)
		})

		ll.assert(len(sampleDataFrames) > 0, "No input files to process in preprocessByCell")

		concatenatedDf := pt.concat(sampleDataFrames)

		// Add rawChainRank column
		dfWithRawRank := concatenatedDf.withColumns(
			pt.rank(pt.col(mainAbundanceColumn), {descending: true}).
				over([pt.col("sampleId"), pt.col("cellKey")]).
				alias("rawChainRank")
		)

		// Filter by rawChainRank <= 2
		dfFilteredByRawRank := dfWithRawRank.filter(
			pt.col("rawChainRank").le(2)
		)

		// Prepare expressions for chainRank ordering
        // 1. isProductiveNumeric: true -> 0, false -> 1 (case-insensitive)
        isProductiveNumericExpr := pt.when(pt.col(mainIsProductiveColumn).strToUpper().eq("TRUE")).
			then(pt.lit(0)).
			otherwise(pt.lit(1))

		// 2. negativeAbundance: mainAbundanceColumn * -1 (for ascending sort by this, meaning descending by original abundance)
		negativeAbundanceExpr := pt.col(mainAbundanceColumn).multiply(-1)

		// Add chainRank column and remove temp column rawChainRank
		dfWithChainRank := dfFilteredByRawRank.withColumns(
			pt.rank([
					isProductiveNumericExpr,
					negativeAbundanceExpr
				], {descending: false}).
				over([pt.col("sampleId"), pt.col("cellKey")]).
				alias("chainRank")
		).withoutColumns("rawChainRank")

		dfWithChainRank.save("output.tsv")

		// Run the workflow
		runResult := wf.run()
		return runResult.getFile("output.tsv")
	}

	chainAOutput := preprocessByCell(byCellTagA)
	chainBOutput := preprocessByCell(byCellTagB)

	//
	// Cell grouping - Reimplemented with PTabler pt API
	//
	cellGroupingWf := pt.workflow()

	chainATableDf := cellGroupingWf.frame(chainAOutput, {xsvType: "tsv"})
	chainBTableDf := cellGroupingWf.frame(chainBOutput, {xsvType: "tsv"})

	// Dynamically generate filter steps for chains A and B, ranks 1 and 2
	// Store resulting DataFrames in a map for easier access
	rankedDfs := {}
	for chainData in [{prefix: "a", df: chainATableDf}, {prefix: "b", df: chainBTableDf}] {
		for rankVal in [1, 2] {
			dfKey := "chain_" + chainData.prefix + "_rank" + string(rankVal) + "_df"
			rankedDfs[dfKey] = chainData.df.filter(pt.col("chainRank").eq(rankVal))
		}
	}

	chainAMergedDf := rankedDfs["chain_a_rank1_df"].join(rankedDfs["chain_a_rank2_df"], {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true,
		leftColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyA1" }],
		rightColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyA2" }]
	})

	chainBMergedDf := rankedDfs["chain_b_rank1_df"].join(rankedDfs["chain_b_rank2_df"], {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true,
		leftColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyB1" }],
		rightColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyB2" }]
	})

	allChainsMergedDf := chainAMergedDf.join(chainBMergedDf, {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true
	})

	scClonotypeKeyExpr := pt.concatStr(
		[
			pt.col("clonotypeKeyA1").fillNull("NA"),
			pt.col("clonotypeKeyA2").fillNull("NA"),
			pt.col("clonotypeKeyB1").fillNull("NA"),
			pt.col("clonotypeKeyB2").fillNull("NA")
		],
		{delimiter: "#"}
	).hash("sha256", "base64_alphanumeric", 120).alias("scClonotypeKey")

	allChainsMergedWithScKeyDf := allChainsMergedDf.withColumns(scClonotypeKeyExpr)

	filterCondition := pt.and(
		pt.col("clonotypeKeyA1").isNotNull(),
		pt.col("clonotypeKeyB1").isNotNull()
	)
	allChainsFilteredDf := allChainsMergedWithScKeyDf.filter(filterCondition)

	allChainsFilteredDf.
		withColumns(pt.lit(1).alias("1")).
		save("cells.tsv", {columns: ["sampleId", "cellKey", "scClonotypeKey", "1"]})

	clonotypeTableDf := allChainsFilteredDf.groupBy(
		"scClonotypeKey", "clonotypeKeyA1", "clonotypeKeyA2", "clonotypeKeyB1", "clonotypeKeyB2"
	).agg(
		pt.col("sampleId").nUnique().alias("sampleCount")
	)

	clonotypeTableDf = clonotypeLabel.addClonotypeLabelColumnsPt(clonotypeTableDf, "scClonotypeKey", "clonotypeLabel", pt)

	clonotypeTableDf.save("clonotype.tsv")

	cellCountsDf := allChainsFilteredDf.groupBy("sampleId", "scClonotypeKey").agg(
		pt.col("cellKey").count().alias("uniqueCellCount")
	)

	cellCountsWithFractionDf := cellCountsDf.withColumns(
		pt.col("uniqueCellCount").truediv(
			pt.col("uniqueCellCount").sum().over("sampleId")
		).alias("uniqueCellFraction")
	)

	cellCountsWithFractionDf.save("abundance.tsv")

	// Create linker dataframe from cellCountsWithFractionDf with just sampleId, scClonotypeKey, and link=1
	linkerDf := cellCountsWithFractionDf.withColumns(
		pt.lit(1).alias("link")
	).select(
		pt.col("sampleId"), pt.col("scClonotypeKey"), pt.col("link")
	)
	linkerDf.save("linker.tsv")

	cellGroupingRunResult := cellGroupingWf.run()

	clonotypeTsv := cellGroupingRunResult.getFile("clonotype.tsv")
	abundanceTsv := cellGroupingRunResult.getFile("abundance.tsv")
	linkerTsv := cellGroupingRunResult.getFile("linker.tsv")
	cellsTsv := cellGroupingRunResult.getFile("cells.tsv")

	//
	// Output processing - Reimplemented with PTabler pt API
	//

	propertiesAFile := propertiesA.inputs()["[]"]
	propertiesBFile := propertiesB.inputs()["[]"]

	outputProcessingWf := pt.workflow()

	// Schema for clonotype.tsv (main_clonotypes table)
	clonotypeTableSchema := [
		{ column: "scClonotypeKey", type: "String" },
		{ column: "clonotypeKeyA1", type: "String" },
		{ column: "clonotypeKeyA2", type: "String" },
		{ column: "clonotypeKeyB1", type: "String" },
		{ column: "clonotypeKeyB2", type: "String" },
		{ column: "sampleCount", type: "Int" }
	]

	mainClonotypesDf := outputProcessingWf.frame(clonotypeTsv, {
		xsvType: "tsv",
		schema: clonotypeTableSchema,
		inferSchema: false
	})

	propsASchema := [{ column: "clonotypeKey", type: "String" }]
	propsADf := outputProcessingWf.frame(propertiesAFile, {
		xsvType: "tsv",
		schema: propsASchema,
		inferSchema: false
	})

	propsBSchema := [{ column: "clonotypeKey", type: "String" }]
	propsBDf := outputProcessingWf.frame(propertiesBFile, {
		xsvType: "tsv",
		id: "props_b",
		schema: propsBSchema,
		inferSchema: false
	})


	clonotypeColumnNames := []
	for cc in schemaPerClonotypeNoAggregates {
		clonotypeColumnNames = append(clonotypeColumnNames, cc.column)
	}

	finalOutputColumns := ["scClonotypeKey", "clonotypeKey"] + clonotypeColumnNames


	chainMappings := [
		{ chainKeyCol: "clonotypeKeyA1", propsTable: "props_a", internalOutTable: "props_a1_joined", finalOutFile: "properties_a_primary.tsv" },
		{ chainKeyCol: "clonotypeKeyA2", propsTable: "props_a", internalOutTable: "props_a2_joined", finalOutFile: "properties_a_secondary.tsv" },
		{ chainKeyCol: "clonotypeKeyB1", propsTable: "props_b", internalOutTable: "props_b1_joined", finalOutFile: "properties_b_primary.tsv" },
		{ chainKeyCol: "clonotypeKeyB2", propsTable: "props_b", internalOutTable: "props_b2_joined", finalOutFile: "properties_b_secondary.tsv" }
	]

	propsMapDfs := {
		"props_a": propsADf,
		"props_b": propsBDf
	}

	for mapping in chainMappings {
		filteredClonotypesDf := mainClonotypesDf.filter(
			pt.col(mapping.chainKeyCol).isNotNull()
		)

		filteredClonotypesWithKeyDf := filteredClonotypesDf.withColumns(
			pt.col(mapping.chainKeyCol).alias("clonotypeKey")
		)

		leftDfForJoin := propsMapDfs[mapping.propsTable]

		joinedDf := leftDfForJoin.join(filteredClonotypesWithKeyDf, {
			how: "inner",
			on: ["clonotypeKey"]
		})

		joinedDf.save(mapping.finalOutFile, {
			columns: finalOutputColumns,
			xsvType: "tsv"
		})
	}

	outputProcessingRunResult := outputProcessingWf.run()

	return {
		// must have sampleId and scClonotypeKey columns
		abundanceTsv: abundanceTsv,

		// used for aggregates (i.e. sampleCount and clonotypeLabel)
		clonotypeTsv: clonotypeTsv,

		// used to build cell <-> clonotype linker column
		cellsTsv: cellsTsv,

		// linker table with sampleId, scClonotypeKey, and link columns
		linkerTsv: linkerTsv,

		// must have scClonotypeKey columns
		propertiesAPrimaryTsv: outputProcessingRunResult.getFile(chainMappings[0].finalOutFile),
		propertiesASecondaryTsv: outputProcessingRunResult.getFile(chainMappings[1].finalOutFile),
		propertiesBPrimaryTsv: outputProcessingRunResult.getFile(chainMappings[2].finalOutFile),
		propertiesBSecondaryTsv: outputProcessingRunResult.getFile(chainMappings[3].finalOutFile)
	}
})
